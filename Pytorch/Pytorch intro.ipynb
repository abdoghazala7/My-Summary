{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a6d2dd",
   "metadata": {},
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a8f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f42a5",
   "metadata": {},
   "source": [
    "# Make torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f0c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3795, -0.2128,  0.4384],\n",
       "        [-0.3576, -1.0092,  0.2302],\n",
       "        [-1.4584,  0.8828,  0.8090]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff8753fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3795, -0.2128,  0.4384, -0.3576, -1.0092,  0.2302, -1.4584,  0.8828,\n",
       "         0.8090])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf29355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1,2,3])\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f68c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df4adf",
   "metadata": {},
   "source": [
    "# torch grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed8c871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([1.,2.,3.] ,requires_grad=True) # the numbers in tensor with grad  should be float \n",
    "z = w *2\n",
    "l = z.sum()\n",
    "l.backward()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6423d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9fc149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x1125435eb30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f3920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x1125435f220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522ac901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dfeac7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "l.backward() # we should use requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e01ba40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([1.,2.,3.] ,requires_grad=True)\n",
    "w.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf3bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c4c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.detach_() # like requires_grad_(False) make overwriting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa4e6c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d70b9b",
   "metadata": {},
   "source": [
    "when we write _ in any function it make overwriting like implace = True in pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d20e59",
   "metadata": {},
   "source": [
    "# torch backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe051d0",
   "metadata": {},
   "source": [
    "i will make a small linear regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a904206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0000, requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor(1.)\n",
    "y= torch.tensor(2.)\n",
    "w = torch.tensor(1. , requires_grad=True)\n",
    "\n",
    "for _ in range(500) :\n",
    "    \n",
    "    ypred = w * x\n",
    "    loss = (ypred-y)**2\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w-= .01 * w.grad\n",
    "        \n",
    "    w.grad.zero_()    \n",
    "w   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31e138f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PowBackward0 at 0x112572ebc70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "465db3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x112572eb700>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21939c8f",
   "metadata": {},
   "source": [
    "# make Neoralnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cad079",
   "metadata": {},
   "source": [
    "i will make data random for X , Y and the neoralnetwork have 2 hidden layers , for the first have 30 unit and second have 16 and the output layer have 10 unit and the activation fun here is Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8405051",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((16,1000))\n",
    "y = torch.randn((16,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86da36",
   "metadata": {},
   "source": [
    "### make forward class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5352e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3af7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeoralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeoralNet , self).__init__()\n",
    "        \n",
    "        self.L1 = nn.Linear(1000,30)  # linear here like Dense in Tensorflow \n",
    "        self.L2 = nn.Linear(30,16)\n",
    "        self.L3 = nn.Linear(16,10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x = self.L1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L3(x)\n",
    "        \n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e150987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeoralNet()   # like import model from sckit learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedff9b7",
   "metadata": {},
   "source": [
    "### Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5186d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d28c0",
   "metadata": {},
   "source": [
    "### optimizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ea48d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ab0f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters() , lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb089e",
   "metadata": {},
   "source": [
    "leet see the intialziation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc797c99",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1117, -0.1557,  0.1228, -0.0109, -0.1000,  0.0790, -0.0570,  0.1103,\n",
       "         -0.0860,  0.0905,  0.0891, -0.0370, -0.0740, -0.1579, -0.1356,  0.1348,\n",
       "         -0.1376, -0.0195,  0.0218,  0.0714, -0.1075,  0.1010, -0.0824,  0.1309,\n",
       "          0.1186, -0.1012, -0.0366, -0.0548, -0.0071, -0.0898],\n",
       "        [-0.0506,  0.1765, -0.0707, -0.1395, -0.0547,  0.1141,  0.1417,  0.1519,\n",
       "          0.0456, -0.0594, -0.0037,  0.1815, -0.1616,  0.0058, -0.0739,  0.0047,\n",
       "         -0.0552,  0.1819, -0.0455, -0.0696, -0.0676, -0.0986,  0.0269, -0.0808,\n",
       "          0.1052,  0.1184, -0.1218, -0.0170,  0.0293, -0.1010],\n",
       "        [ 0.0245, -0.0611, -0.0009,  0.0696,  0.0558,  0.1441, -0.0950,  0.1322,\n",
       "         -0.1350,  0.0309,  0.0820,  0.0777,  0.1818,  0.0711, -0.1243,  0.1279,\n",
       "         -0.0050, -0.0863, -0.0110, -0.0016, -0.1761, -0.0271,  0.1192,  0.0133,\n",
       "         -0.1582,  0.1654, -0.0797,  0.0835,  0.0603, -0.0693],\n",
       "        [ 0.0080,  0.0812, -0.0921, -0.1162,  0.0185,  0.1453,  0.1189,  0.0105,\n",
       "         -0.0783, -0.0113, -0.1720,  0.0409,  0.0815,  0.0783, -0.1316, -0.1257,\n",
       "          0.0628, -0.0027, -0.1085,  0.0918, -0.0731,  0.1001,  0.1784, -0.0449,\n",
       "         -0.0207, -0.0037,  0.1729,  0.1744,  0.1171,  0.1446],\n",
       "        [ 0.0605,  0.0997,  0.0462,  0.0472,  0.1542,  0.0216, -0.0144, -0.0488,\n",
       "         -0.1704, -0.0119,  0.1633,  0.1318, -0.1439, -0.0098,  0.1591, -0.0785,\n",
       "          0.0979, -0.1095,  0.1770,  0.1070,  0.0553, -0.0537, -0.0967, -0.0072,\n",
       "          0.0436, -0.0930, -0.1808, -0.0971,  0.1015,  0.0365],\n",
       "        [-0.1351, -0.0174,  0.1807, -0.0379,  0.1038,  0.1022,  0.0715, -0.0159,\n",
       "         -0.0837,  0.0010, -0.0143,  0.1421, -0.0641,  0.1189, -0.0021, -0.0004,\n",
       "          0.1786, -0.1740, -0.1185, -0.0678, -0.1341,  0.1624, -0.1027, -0.0714,\n",
       "          0.1311,  0.0296,  0.0674,  0.0387,  0.0288,  0.1221],\n",
       "        [ 0.0543,  0.0077, -0.0485,  0.0298,  0.1020, -0.1746, -0.1277,  0.1378,\n",
       "          0.1744,  0.0117,  0.0438,  0.0346, -0.0189,  0.0068, -0.0509,  0.1086,\n",
       "          0.1152,  0.0361, -0.1300,  0.1053, -0.0027, -0.1640, -0.0074,  0.0804,\n",
       "         -0.0377, -0.0242,  0.0713,  0.0635, -0.1595,  0.0730],\n",
       "        [ 0.0423,  0.1645, -0.1463, -0.0390,  0.0429,  0.0500, -0.1145,  0.1437,\n",
       "         -0.0989,  0.0460,  0.1203,  0.0820, -0.0451,  0.1146, -0.0954,  0.0029,\n",
       "         -0.0093, -0.1319, -0.0600, -0.0789, -0.0981,  0.1304,  0.0081,  0.0328,\n",
       "          0.1472,  0.0752, -0.1777, -0.0134,  0.1784,  0.1365],\n",
       "        [-0.1255, -0.0488,  0.0580, -0.1445, -0.1503,  0.0451, -0.0638, -0.1559,\n",
       "          0.1786,  0.0221,  0.0571,  0.1678, -0.0511, -0.1431, -0.1732, -0.1787,\n",
       "         -0.0982, -0.0293, -0.0478,  0.0481,  0.1795, -0.1260,  0.1313, -0.0393,\n",
       "          0.0347, -0.1152, -0.1447, -0.0212, -0.1521, -0.0236],\n",
       "        [-0.0953, -0.1368,  0.0238, -0.0653,  0.0059, -0.1361, -0.0449,  0.1469,\n",
       "          0.0686,  0.1060, -0.0341,  0.0397,  0.1157, -0.0935, -0.1331,  0.0279,\n",
       "          0.0926, -0.1538, -0.0934, -0.1461, -0.0259,  0.0704, -0.0895,  0.0852,\n",
       "         -0.0286,  0.1662, -0.0526, -0.0187,  0.0444, -0.0753],\n",
       "        [ 0.0553,  0.1232, -0.1730,  0.0326,  0.0247, -0.1683,  0.0072,  0.1215,\n",
       "          0.0636, -0.0527, -0.1757, -0.0877, -0.0776, -0.1229, -0.0440,  0.1572,\n",
       "         -0.0489, -0.0394,  0.0161,  0.0835, -0.0585, -0.0995, -0.1690, -0.1274,\n",
       "          0.0724,  0.0916,  0.0951,  0.0484, -0.0059, -0.1730],\n",
       "        [ 0.0811, -0.1424, -0.0344,  0.0339, -0.0784,  0.0648,  0.0692, -0.1712,\n",
       "          0.1808, -0.0216,  0.0781, -0.1229, -0.0352, -0.0491,  0.0712, -0.0867,\n",
       "          0.0923, -0.0386, -0.1270,  0.0659, -0.1812, -0.0501,  0.1249,  0.1806,\n",
       "          0.1151,  0.0837,  0.0737,  0.0932,  0.0111,  0.0506],\n",
       "        [ 0.0386, -0.0896, -0.0220,  0.0818, -0.1454, -0.1068,  0.0856, -0.1625,\n",
       "         -0.0817,  0.0962, -0.1525, -0.0928, -0.0601,  0.1582, -0.0919,  0.1732,\n",
       "         -0.1106,  0.0781, -0.1658, -0.0470,  0.1190, -0.0100, -0.1390, -0.0104,\n",
       "          0.1644,  0.1496,  0.0971, -0.1593,  0.1066,  0.1042],\n",
       "        [ 0.0127, -0.1459, -0.0813, -0.0940,  0.0812,  0.1669,  0.0838, -0.0773,\n",
       "          0.0860,  0.0289, -0.0665,  0.0272, -0.0117,  0.0682,  0.0086, -0.1242,\n",
       "         -0.1687, -0.0787, -0.0273,  0.1696,  0.1117, -0.1374, -0.1092, -0.1718,\n",
       "         -0.1397,  0.0957, -0.1190,  0.1131,  0.1303, -0.0851],\n",
       "        [-0.1446, -0.0858,  0.1113,  0.0171,  0.1074, -0.1380,  0.0211, -0.1386,\n",
       "         -0.1047, -0.0107, -0.1387, -0.0635, -0.0250,  0.0070,  0.1272, -0.0781,\n",
       "          0.1696, -0.0287, -0.0006, -0.0659, -0.1673,  0.0836, -0.0941,  0.1294,\n",
       "         -0.0072,  0.0989,  0.1249,  0.1819,  0.0429, -0.1447],\n",
       "        [ 0.0726, -0.0777, -0.1785, -0.1668,  0.1082,  0.1498,  0.1732, -0.0652,\n",
       "         -0.0169,  0.1417,  0.0437, -0.0256,  0.0394, -0.1162, -0.0428, -0.0618,\n",
       "          0.1493, -0.0478, -0.0858,  0.0825,  0.1063, -0.1461, -0.0056,  0.0604,\n",
       "         -0.1056, -0.1021,  0.0388,  0.0110,  0.1658, -0.0348]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.L2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb001f58",
   "metadata": {},
   "source": [
    "# training loop(backword) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ee158e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[9]/1000 , loss= 1.0802\n",
      "epoch[19]/1000 , loss= 1.0336\n",
      "epoch[29]/1000 , loss= 0.9909\n",
      "epoch[39]/1000 , loss= 0.9441\n",
      "epoch[49]/1000 , loss= 0.9008\n",
      "epoch[59]/1000 , loss= 0.8593\n",
      "epoch[69]/1000 , loss= 0.8172\n",
      "epoch[79]/1000 , loss= 0.7751\n",
      "epoch[89]/1000 , loss= 0.7345\n",
      "epoch[99]/1000 , loss= 0.6943\n",
      "epoch[109]/1000 , loss= 0.6556\n",
      "epoch[119]/1000 , loss= 0.6181\n",
      "epoch[129]/1000 , loss= 0.5814\n",
      "epoch[139]/1000 , loss= 0.5445\n",
      "epoch[149]/1000 , loss= 0.5084\n",
      "epoch[159]/1000 , loss= 0.4735\n",
      "epoch[169]/1000 , loss= 0.4405\n",
      "epoch[179]/1000 , loss= 0.4098\n",
      "epoch[189]/1000 , loss= 0.3818\n",
      "epoch[199]/1000 , loss= 0.3565\n",
      "epoch[209]/1000 , loss= 0.3339\n",
      "epoch[219]/1000 , loss= 0.3135\n",
      "epoch[229]/1000 , loss= 0.2953\n",
      "epoch[239]/1000 , loss= 0.2785\n",
      "epoch[249]/1000 , loss= 0.2636\n",
      "epoch[259]/1000 , loss= 0.2504\n",
      "epoch[269]/1000 , loss= 0.2385\n",
      "epoch[279]/1000 , loss= 0.2278\n",
      "epoch[289]/1000 , loss= 0.2178\n",
      "epoch[299]/1000 , loss= 0.2087\n",
      "epoch[309]/1000 , loss= 0.2002\n",
      "epoch[319]/1000 , loss= 0.1923\n",
      "epoch[329]/1000 , loss= 0.1849\n",
      "epoch[339]/1000 , loss= 0.1779\n",
      "epoch[349]/1000 , loss= 0.1712\n",
      "epoch[359]/1000 , loss= 0.1649\n",
      "epoch[369]/1000 , loss= 0.1591\n",
      "epoch[379]/1000 , loss= 0.1536\n",
      "epoch[389]/1000 , loss= 0.1485\n",
      "epoch[399]/1000 , loss= 0.1437\n",
      "epoch[409]/1000 , loss= 0.1392\n",
      "epoch[419]/1000 , loss= 0.1346\n",
      "epoch[429]/1000 , loss= 0.1303\n",
      "epoch[439]/1000 , loss= 0.1263\n",
      "epoch[449]/1000 , loss= 0.1226\n",
      "epoch[459]/1000 , loss= 0.1190\n",
      "epoch[469]/1000 , loss= 0.1156\n",
      "epoch[479]/1000 , loss= 0.1125\n",
      "epoch[489]/1000 , loss= 0.1094\n",
      "epoch[499]/1000 , loss= 0.1066\n",
      "epoch[509]/1000 , loss= 0.1038\n",
      "epoch[519]/1000 , loss= 0.1012\n",
      "epoch[529]/1000 , loss= 0.0988\n",
      "epoch[539]/1000 , loss= 0.0964\n",
      "epoch[549]/1000 , loss= 0.0941\n",
      "epoch[559]/1000 , loss= 0.0919\n",
      "epoch[569]/1000 , loss= 0.0898\n",
      "epoch[579]/1000 , loss= 0.0878\n",
      "epoch[589]/1000 , loss= 0.0858\n",
      "epoch[599]/1000 , loss= 0.0840\n",
      "epoch[609]/1000 , loss= 0.0822\n",
      "epoch[619]/1000 , loss= 0.0804\n",
      "epoch[629]/1000 , loss= 0.0788\n",
      "epoch[639]/1000 , loss= 0.0772\n",
      "epoch[649]/1000 , loss= 0.0756\n",
      "epoch[659]/1000 , loss= 0.0741\n",
      "epoch[669]/1000 , loss= 0.0727\n",
      "epoch[679]/1000 , loss= 0.0713\n",
      "epoch[689]/1000 , loss= 0.0699\n",
      "epoch[699]/1000 , loss= 0.0686\n",
      "epoch[709]/1000 , loss= 0.0674\n",
      "epoch[719]/1000 , loss= 0.0661\n",
      "epoch[729]/1000 , loss= 0.0650\n",
      "epoch[739]/1000 , loss= 0.0638\n",
      "epoch[749]/1000 , loss= 0.0627\n",
      "epoch[759]/1000 , loss= 0.0616\n",
      "epoch[769]/1000 , loss= 0.0606\n",
      "epoch[779]/1000 , loss= 0.0596\n",
      "epoch[789]/1000 , loss= 0.0585\n",
      "epoch[799]/1000 , loss= 0.0576\n",
      "epoch[809]/1000 , loss= 0.0566\n",
      "epoch[819]/1000 , loss= 0.0557\n",
      "epoch[829]/1000 , loss= 0.0548\n",
      "epoch[839]/1000 , loss= 0.0539\n",
      "epoch[849]/1000 , loss= 0.0531\n",
      "epoch[859]/1000 , loss= 0.0523\n",
      "epoch[869]/1000 , loss= 0.0514\n",
      "epoch[879]/1000 , loss= 0.0506\n",
      "epoch[889]/1000 , loss= 0.0499\n",
      "epoch[899]/1000 , loss= 0.0491\n",
      "epoch[909]/1000 , loss= 0.0484\n",
      "epoch[919]/1000 , loss= 0.0477\n",
      "epoch[929]/1000 , loss= 0.0470\n",
      "epoch[939]/1000 , loss= 0.0463\n",
      "epoch[949]/1000 , loss= 0.0457\n",
      "epoch[959]/1000 , loss= 0.0450\n",
      "epoch[969]/1000 , loss= 0.0444\n",
      "epoch[979]/1000 , loss= 0.0438\n",
      "epoch[989]/1000 , loss= 0.0432\n",
      "epoch[999]/1000 , loss= 0.0426\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = Loss(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(epoch+1) % 10 == 0 :\n",
    "        print(f'epoch[{epoch}]/{num_epochs} , loss= {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f56420a2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1159, -0.1562,  0.1262, -0.0273, -0.0871,  0.1191, -0.0741,  0.1739,\n",
       "         -0.0881,  0.0732,  0.0715, -0.0554, -0.0592, -0.1666, -0.1352,  0.1716,\n",
       "         -0.1339, -0.0190,  0.0106,  0.0960, -0.1469,  0.0921, -0.0821,  0.1303,\n",
       "          0.1013, -0.1029, -0.0395, -0.0176,  0.0230, -0.0871],\n",
       "        [-0.0335,  0.2275, -0.1055, -0.1109, -0.0058,  0.2362,  0.2571,  0.2244,\n",
       "          0.0192, -0.0414, -0.0030,  0.1180, -0.1441,  0.0033, -0.0319,  0.0459,\n",
       "          0.0189,  0.2290, -0.0380, -0.1189, -0.0546, -0.0875,  0.0777, -0.0833,\n",
       "          0.1173,  0.1843, -0.1127,  0.0943, -0.0142, -0.1228],\n",
       "        [ 0.0382, -0.0437,  0.0253,  0.0567,  0.0975,  0.1783, -0.1129,  0.1766,\n",
       "         -0.1410,  0.0666,  0.0802,  0.0874,  0.2051,  0.0500, -0.0965,  0.1744,\n",
       "          0.0354, -0.0909, -0.0330, -0.0371, -0.1984, -0.0271,  0.1223,  0.0184,\n",
       "         -0.1438,  0.2098, -0.0819,  0.1703,  0.1494, -0.0919],\n",
       "        [-0.0144,  0.0543, -0.0892, -0.1271,  0.0227,  0.2211,  0.1274,  0.0055,\n",
       "         -0.0464, -0.0525, -0.1743,  0.0781,  0.1465,  0.1193, -0.2554, -0.0979,\n",
       "         -0.0016,  0.0543, -0.1000,  0.2125, -0.1222,  0.1091,  0.2532, -0.0493,\n",
       "          0.0156,  0.0269,  0.2620,  0.1975,  0.2268,  0.1830],\n",
       "        [ 0.1043,  0.1504,  0.0619,  0.0778,  0.2358,  0.0298, -0.0326,  0.0212,\n",
       "         -0.1739, -0.0340,  0.1841,  0.1554, -0.1250,  0.0191,  0.1666, -0.0374,\n",
       "          0.1332, -0.0530,  0.2115,  0.1799,  0.0709, -0.0945,  0.0007, -0.0020,\n",
       "          0.0894, -0.0739, -0.1763, -0.0515,  0.1556,  0.1073],\n",
       "        [-0.0851, -0.0412,  0.2616, -0.0632,  0.1010,  0.2485,  0.1219,  0.1177,\n",
       "         -0.0528,  0.0074, -0.0280,  0.2513, -0.0647,  0.1174,  0.0985,  0.0204,\n",
       "          0.2127, -0.2259, -0.1402, -0.0843, -0.1062,  0.2003, -0.1217, -0.0752,\n",
       "          0.1847,  0.0563,  0.0871,  0.0766,  0.0359,  0.1906],\n",
       "        [ 0.0294,  0.0121, -0.0275,  0.0393,  0.2108, -0.1833, -0.1247,  0.1795,\n",
       "          0.2325,  0.0151,  0.0871,  0.0622, -0.0247, -0.0259,  0.0354,  0.1442,\n",
       "          0.1033,  0.0040, -0.1271,  0.1906, -0.0046, -0.1696,  0.0326,  0.0768,\n",
       "         -0.0202, -0.0262,  0.0627,  0.0593, -0.2193,  0.2205],\n",
       "        [ 0.0874,  0.1654, -0.1634, -0.0349,  0.0075,  0.0172, -0.1174,  0.1389,\n",
       "         -0.0889,  0.0484,  0.1244,  0.0660, -0.0513,  0.1390, -0.0611, -0.0170,\n",
       "          0.0431, -0.1757, -0.0579, -0.0925, -0.0661,  0.0893, -0.0246,  0.0346,\n",
       "          0.1386,  0.0543, -0.1832, -0.0232,  0.1299,  0.1462],\n",
       "        [-0.1255, -0.0551,  0.0792, -0.1472, -0.1606,  0.0597, -0.0552, -0.1557,\n",
       "          0.1949,  0.0228,  0.0575,  0.2070, -0.0506, -0.1409, -0.1755, -0.1840,\n",
       "         -0.1024, -0.0295, -0.0499,  0.0332,  0.1764, -0.1207,  0.1311, -0.0391,\n",
       "          0.0589, -0.1055, -0.1351, -0.0152, -0.1566, -0.0292],\n",
       "        [-0.0953, -0.1365,  0.0415, -0.0548, -0.0048, -0.1768, -0.0352,  0.1522,\n",
       "          0.0505,  0.1555, -0.0247,  0.0600,  0.1154, -0.0941, -0.1486,  0.0876,\n",
       "          0.1143, -0.1535, -0.0932, -0.1556, -0.0058,  0.0643, -0.0913,  0.1026,\n",
       "          0.0020,  0.2247, -0.0559, -0.0430,  0.1469, -0.0727],\n",
       "        [ 0.0528,  0.1077, -0.1495,  0.0451,  0.0330, -0.1025,  0.0382,  0.2315,\n",
       "          0.0805, -0.0553, -0.1747, -0.0783, -0.0785, -0.1244, -0.0572,  0.2388,\n",
       "         -0.0663, -0.0417,  0.0271,  0.1087, -0.0480, -0.1006, -0.1774, -0.1270,\n",
       "          0.0885,  0.0979,  0.0954,  0.0674,  0.0118, -0.1768],\n",
       "        [ 0.0614, -0.1407, -0.0338,  0.0326, -0.0705,  0.0278,  0.0630, -0.1649,\n",
       "          0.1643, -0.0614,  0.0721, -0.1105, -0.0648, -0.0803,  0.0965, -0.0900,\n",
       "          0.1116, -0.0627, -0.1189,  0.0808, -0.1952, -0.0536,  0.1566,  0.1805,\n",
       "          0.1295,  0.0992,  0.0235,  0.1074, -0.0035,  0.0885],\n",
       "        [ 0.0411, -0.0934, -0.0071,  0.1023, -0.1358, -0.1541,  0.0789, -0.1364,\n",
       "         -0.0772,  0.0763, -0.1519, -0.0749, -0.0542,  0.1786, -0.1030,  0.2257,\n",
       "         -0.1105,  0.0926, -0.1470, -0.0229,  0.1012, -0.0041, -0.1263, -0.0063,\n",
       "          0.1994,  0.1866,  0.0967, -0.1596,  0.1362,  0.1128],\n",
       "        [ 0.0013, -0.1547, -0.0590, -0.0953,  0.1069,  0.2736,  0.1307, -0.0102,\n",
       "          0.1041,  0.1208, -0.0815, -0.0024, -0.0142,  0.1217,  0.0346, -0.1082,\n",
       "         -0.1576, -0.0495, -0.0242,  0.2165,  0.1768, -0.1152, -0.1010, -0.1627,\n",
       "         -0.1647,  0.0806, -0.0898,  0.1930,  0.2103, -0.1320],\n",
       "        [-0.1835, -0.0859,  0.1336,  0.0230,  0.2049, -0.1311,  0.0512, -0.1602,\n",
       "         -0.0994,  0.0392, -0.1462, -0.0814, -0.0341,  0.0031,  0.2336, -0.0807,\n",
       "          0.2203, -0.0287, -0.0015, -0.0678, -0.1565,  0.1168, -0.0568,  0.1284,\n",
       "         -0.0035,  0.1230,  0.1097,  0.2919,  0.0619, -0.1650],\n",
       "        [ 0.1199, -0.0610, -0.1798, -0.1683,  0.1467,  0.2873,  0.1979, -0.0701,\n",
       "         -0.0186,  0.2004,  0.0624, -0.0288,  0.0572, -0.1568, -0.0378, -0.0597,\n",
       "          0.1595, -0.0689, -0.0914,  0.0280,  0.1864, -0.1691, -0.0569,  0.0566,\n",
       "         -0.1203, -0.1200,  0.0654,  0.0240,  0.1874, -0.0713]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.L2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa9f4f",
   "metadata": {},
   "source": [
    "# if i runing the turining loop again it will began from the last weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
